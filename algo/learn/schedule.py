"""
This module defines the scheduling events.
"""
import numpy as np
import heapq
import itertools
from collections import OrderedDict
from params import args
from spark_env.wall_time import WallTime
from spark_env.executor import Executor, FreeExecutors, MovingExecutors
from job_generator import generate_jobs
from spark_env.task import Task
from spark_env.job import Job
import utils
# args.moving_delay can be replaced to introduce heterogenous communication overhead


class Schedule:
    """
    Define the scheduling events.
    """
    def __init__(self):
        self.np_random = np.random.RandomState()
        self.wall_time = WallTime()
        self.timeline = Timeline()

        self.executors = utils.OrderedSet()
        for exec_idx in range(args.exec_cap):
            self.executors.add(Executor(exec_idx))
        self.free_executors = FreeExecutors(self.executors)
        self.moving_executors = MovingExecutors()
        self.exec_commit = ExecutorCommit()

        # stages wait to be scheduled, these stages are the return of scheduling algorithms
        self.stage_selected = set()
        self.reward_calculator = RewardCalculator()

        self.exec_to_schedule = None                   # executors to be scheduled
        self.src_job = None
        self.num_src_exec = -1                         # number of execs to be scheduled
        self.jobs = None
        self.action_map = None                         # a ReversibleMap from act to stage
        self.finished_jobs = None                      # we track this for updating the action-to-stage map
        self.max_time = None

    def step(self, next_stage, limit):
        """
        One (scheduling event) step forward:
            with given actions as input, submit commitment (invoke scheduling event if necessary),
            and return the reward and the new state.
        :param next_stage: the next to-be-scheduled stage, generated by scheduling algorithm
        :param limit: the exec limit for the job of the next to-be-scheduled stage, generated by scheduling algorithm
        """
        assert next_stage not in self.stage_selected
        self.stage_selected.add(next_stage)

        # note that currently self.exec_to_schedule are all coming from the same src,
        # go find the src to make sure whether (executors') moving delay is necessary or not
        executor = next(iter(self.exec_to_schedule))
        src = executor.job if executor.stage is None else executor.stage

        # calculate how many execs we need to reissue to next_stage
        # if next_stage is None, we just collect all execs in self.exec_to_schedule and save them to self.free_executors
        if next_stage is not None:
            use_exec = min(
                (next_stage.num_tasks - next_stage.next_task_idx) -                                       # next_stage really needs
                (self.exec_commit.stage_commit[next_stage] + self.moving_executors.count(next_stage)),    # next_stage already got
                limit                                                                                     # the upper limit informed by the scheduling algorithm
            )
        else:
            use_exec = limit
        assert use_exec > 0

        # here is the interesting thing... We do not schedule next_stage with use_exec executors at present,
        # but submit the transaction into commitment! So... when we really invoke the scheduling event?
        # When a new scheduling round starts! In that case, we fulfill all the commitments in self.exec_commit
        self.exec_commit.add(src, next_stage, use_exec)
        self.num_src_exec -= use_exec
        assert self.num_src_exec >= 0       # if this not fulfilled, sth. wrong with the limit return from scheduling algorithm

        if self.num_src_exec == 0:
            # invoke the scheduling event!
            self.stage_selected.clear()     # the left selected stages cannot be scheduled any more, start all over again
            self.schedule()

        # now we update to the new state...
        # Pay attention to the second condition of the while loop: only if self.num_src_exec is 0, which means
        # the scheduling event is invoked, we update the state. If scheduling event is not invoked, we do not update state

        # The second condition also means every time self.num_src_exec changes, new commitment will be submitted,
        # which means a next scheduling event is preparing, thus directly exit the while loop
        while len(self.timeline) > 0 and self.num_src_exec == 0:
            new_time, item = self.timeline.pop()
            self.wall_time.update(new_time)

            # according to the type of item, turn into different state
            if isinstance(item, Task):
                # ==== task finish event ====: update stage's finished task num, job' finished stages num, and theirs finish time
                finished_task = item
                stage = finished_task.stage

                stage.num_finished_tasks += 1
                frontier_changed = False
                if stage.num_finished_tasks == stage.num_tasks:
                    assert not stage.all_tasks_done
                    stage.all_tasks_done = True
                    stage.job.num_finished_stages += 1
                    stage.finish_time = self.wall_time.cur_time
                    frontier_changed = stage.job.update_frontier_stages(stage)

                # free up this executor, which may invoke scheduling event directly
                self.redispatch_executor(finished_task.executor, frontier_changed)

                if stage.job.num_finished_stages == stage.job.num_stages:
                    # update job completion status
                    assert not stage.job.finished
                    stage.job.finished = True
                    stage.job.finish_time = self.wall_time.cur_time
                    self.remove_job(stage.job)

            elif isinstance(item, Job):
                # ==== new job arrives event ====: update act map, assign all free execs to the newly arrived job
                job = item
                assert not job.arrived
                job.arrived = True
                self.jobs.add(job)
                self.add_job(job)
                self.action_map = get_act2stage(self.jobs)

                if len(self.free_executors[None]) > 0:
                    self.exec_to_schedule = utils.OrderedSet(self.free_executors[None])
                    self.src_job = None
                    self.num_src_exec = len(self.free_executors[None])

            elif isinstance(item, Executor):
                # ==== executor arrival event ====: bind the exec to the destination job or temporarily store it to the src's free_executors pool
                executor = item
                # get the destination (stage) of this executor
                stage = self.moving_executors.pop(executor)
                if stage is not None:
                    # the job (of this stage) is not yet finished when this executor arrives, bind this 'executor' to its arrived job
                    executor.job = stage.job
                    stage.job.executors.add(executor)
                if stage is not None and not stage.no_more_task:
                    # this stage is schedulable, directly schedule it
                    if stage in stage.job.frontier_stages:
                        # this stage is immediately runnable
                        task = stage.schedule(executor)
                        self.timeline.push(task.finish_time, task)
                    else:
                        # add this executor to the free executor pool of this job
                        self.free_executors.add(executor.job, executor)
                else:
                    # this stage is saturated or this job is finished, but the executor still arrives to it
                    # in this case, use backup schedule policy
                    self.backup_schedule(executor)

            else:
                print('Illegal event type!')
                exit(1)

        # compute reward
        reward = self.reward_calculator.get_reward(self.jobs, self.wall_time.cur_time)
        # no more decision to make: jobs all done or time is up
        done = self.num_src_exec == 0 and (len(self.timeline) == 0 or self.wall_time.cur_time >= self.max_time)
        if done:
            assert self.wall_time.cur_time >= self.max_time or len(self.jobs) == 0

        # return new state, reward and a flag to indicate whether all jobs are finished
        # these return vars are the input of the scheduling algorithm
        return self.observe(), reward, done

    def redispatch_executor(self, executor, frontier_changed):
        """
        Dispatch a finished task's 'executor' to the other stages.
        In this function, scheduling event may be invoked directly without consulting self.exec_commit.
        """
        if executor.stage is not None and not executor.stage.no_more_task:
            # the stage which this executor previously worked on is not finished, just directly schedule this executor to the next task
            # This is wired because every scheduling event should be decided by the agent. However, considering the
            # principle of locality, do it like this can not only improve the efficiency, but also avoid invoking the
            # agent too many times
            task = executor.stage.schedule(executor)
            self.timeline.push(task.finish_time, task)
            return

        # if we run here, it means we need to move executor from executor.stage, but move to where is related
        # to frontier_changed, thus we start a classified discussion
        if frontier_changed:
            src_job = executor.job
            # self.exec_commit[executor.stage] is equal to self.exec_commit.commit[executor.stage] because of the __getitem__ func
            if len(self.exec_commit[executor.stage]) > 0:
                # this condition means that this executor has been decided to be dispatched to some job (or stage),
                # thus we just directly fulfill the exactly commitment
                self.exec_to_schedule = {executor}
                self.schedule()
            else:
                # this executor is not arranged, temporarily store to the previously served job's free pool
                self.free_executors.add(src_job, executor)

            self.exec_to_schedule = utils.OrderedSet(self.free_executors[src_job])
            self.src_job = src_job
            self.num_src_exec = len(self.free_executors[src_job])

        else:
            # only need to schedule this executor
            self.exec_to_schedule = {executor}
            if len(self.exec_commit[executor.stage]) > 0:
                self.schedule()
            else:
                # note that self.exec_to_schedule is immediate, self.num_source_exec is for commit
                # so len(self.exec_to_schedule) != self.num_source_exec can happen!
                self.src_job = executor.job
                self.num_src_exec = len(executor.stage.executors)

    def seed(self, seed):
        self.np_random.seed(seed)

    def add_job(self, job):
        self.moving_executors.add_job(job)
        self.free_executors.add_job(job)
        self.exec_commit.add_job(job)

    def schedule(self):
        """
        Do scheduling as self.exec_commits indicates.
        Each time the schedule() func only schedule execs from only 'one' entry of self.exec_commit.
        In this entry, self.exec_to_schedule all come from the same src.
        """
        executor = next(iter(self.exec_to_schedule))
        src = executor.job if executor.stage is None else executor.stage

        # schedule executors from the src (stage or job) until the commitment is fulfilled
        while len(self.exec_commit[src]) > 0 and len(self.exec_to_schedule) > 0:
            # now we try to allocate 'executor' to 'stage', we need to launch classified discussion
            stage = self.exec_commit.pop(src)
            executor = self.exec_to_schedule.pop()

            # mark the executor as busy because it is scheduled
            if self.free_executors.contain_executor(executor.job, executor):
                self.free_executors.remove(executor)

            if stage is None:
                # no schedule event to do, we can only temporarily store 'executor'
                # if the previously served job of this exec is not yet finished, add to its free pool
                # else add to the global free pool
                if executor.job is not None and any([not s.no_more_task for s in executor.job.stages]):
                    self.free_executors.add(executor.job, executor)
                else:
                    self.free_executors.add(None, executor)

            elif not stage.no_more_task:
                # stage is not None and not finished yet
                if executor.job == stage.job:
                    # 'stage' and the previously served job of 'executor' is the same one,
                    # in this case, no moving delay incurred
                    if stage in stage.job.frontier_stages:
                        # this task is immediately runnable, schedule it
                        task = stage.schedule(executor)
                        self.timeline.push(task.finish_time, task)
                    else:
                        # no schedule event to do, we can only temporarily store 'executor'
                        self.free_executors.add(executor.job, executor)
                else:
                    # we do not schedule here, but push this event into timeline
                    # Why we do not directly schedule this exec here? This is because this schedule event is happen
                    # in future, not now! We save it into timeline and really do the schedule event at that time!
                    self.timeline.push(self.wall_time.cur_time + args.moving_delay, executor)
                    self.moving_executors.add(executor, stage)

            else:
                # stage is not None and finished. However, the scheduling algorithm still allocate the exec to this stage...
                # This is a waste of resource! Thus, we just re-dispatch this exec to another wait-for-execution stage
                self.backup_schedule(executor)

    def backup_schedule(self, executor):
        """
        This func is used as backup policy. We add this because a learned policy in early iterations might
        schedule executors to finished stages, which is a waste the execution resource. This func makes sure
        that all executors are working conservatively. The agent should learn to not rely on this func.
        """
        backup_scheduled = False
        if executor.job is not None:
            # == first attempt ==: try to schedule 'executor' on current job
            for stage in executor.job.frontier_stages:
                if not self.saturated(stage):
                    task = stage.schedule(executor)
                    self.timeline.push(task.finish_time, task)
                    backup_scheduled = True
                    break

        if not backup_scheduled:
            # == second attempt ==: try to schedule it to any available stage currently not finished
            # if succeed, the chosen stage must be different from executor.job, thus, add moving delay
            schedulable_stages = self.get_frontier_stages()
            if len(schedulable_stages) > 0:
                stage = next(iter(schedulable_stages))
                self.timeline.push(self.wall_time.cur_time + args.moving_delay, executor)
                self.moving_executors.add(executor, stage)
                backup_scheduled = True

        if not backup_scheduled:
            # == all attempts filed ==: no available stage, 'executor' is totally idle now
            self.free_executors.add(executor.job, executor)

    def saturated(self, stage):
        """
        A stage is saturated iff it is finished as plan, which means we dont need to schedule any execs to this stage any more.
        """
        expected_task_idx = stage.next_task_idx + self.exec_commit.stage_commit[stage] + self.moving_executors.count(stage)
        return expected_task_idx >= stage.num_tasks

    def get_frontier_stages(self):
        """
        Get all the frontier stages from all currently not finished jobs.
        Distinguish this from each job's job.frontier_stages. This is mainly (only) used for observation.
        In this class, 'frontier' can be interpreted as itself is unsaturated but all its parent stages are saturated.
        """
        frontier_stages = utils.OrderedSet()
        for job in self.jobs:
            for stage in job.stages:
                if stage not in self.stage_selected and not self.saturated(stage):
                    all_parents_saturated = True
                    for parent in stage.parent_stages:
                        if not self.saturated(parent):
                            all_parents_saturated = False
                            break
                    if all_parents_saturated:
                        frontier_stages.add(stage)
        return frontier_stages

    def get_binding_execs_num(self):
        """
        Get currently the num of binding execs to each job.
        """
        exec_lmt = {}              # {job: int}
        for job in self.jobs:
            if self.src_job == job:
                exec_lmt[job] = len(job.executors) - self.num_src_exec
            else:
                exec_lmt[job] = len(job.executors)
        return exec_lmt

    def observe(self):
        """
        What this func returns is the observation of current state.
        This observation is used as the input to the agent. The agent construct the
        feature matrix of this observation (state) and output a proper action.
        """
        return self.jobs, self.src_job, self.num_src_exec, self.get_frontier_stages(), \
            self.get_binding_execs_num(), self.exec_commit, self.moving_executors, self.action_map

    def remove_job(self, job):
        """
        Remove the job when it is finished. Only called when update state and when the job is finished.
        """
        for executor in list(job.executors):
            executor.detach_job()
        self.exec_commit.remove_job(job)
        self.free_executors.remove_job(job)
        self.moving_executors.remove_job(job)
        self.jobs.remove(job)
        self.finished_jobs.add(job)
        self.action_map = get_act2stage(self.jobs)

    def reset(self, max_time=np.inf):
        self.max_time = max_time
        self.wall_time.reset()
        self.timeline.reset()
        self.exec_commit.reset()
        self.moving_executors.reset()
        self.reward_calculator.reset()
        self.finished_jobs = utils.OrderedSet()
        self.stage_selected.clear()
        for executor in self.executors:
            executor.reset()
        self.free_executors.reset(self.executors)

        # regenerate jobs (note that self.jobs are only currently arrived jobs)
        self.jobs = generate_jobs(self.np_random, self.timeline, self.wall_time)
        self.action_map = get_act2stage(self.jobs)
        for job in self.jobs:
            self.add_job(job)
        self.src_job = None
        # all executors are schedulable
        self.num_src_exec = len(self.executors)
        self.exec_to_schedule = utils.OrderedSet(self.executors)


class Timeline:
    """
    Store the tuple (time, counter, job/task/executor).
    The pair could be
        - the time a new job arrives;
        - the time a task is finished;
        - the time an executor arrives to anther job (this happens only when we moving executors, i.e. moving delay incurred).
    """
    def __init__(self):
        """
        self.pq (priority queue) stores the tuple: (key, counter, item).
        """
        self.pq = []
        self.counter = itertools.count()      # count starts from 0

    def __len__(self):
        return len(self.pq)

    def push(self, key, item):
        heapq.heappush(self.pq, (key, next(self.counter), item))

    def pop(self):
        """
        Pop the first (key, item) pair from the heap.
        """
        if len(self.pq) > 0:
            key, _, item = heapq.heappop(self.pq)
            return key, item
        return None, None

    def reset(self):
        self.pq = []
        self.counter = itertools.count()


class RewardCalculator:
    """
    Use the execution time for now to calculate the reward.
    For every job still in system, reward will add the negative of the job's executing time till now (of course with scaled factor).
    Obviously, longer each job's execution time, more punishment the Agent receives.
    """
    def __init__(self):
        self.jobs = set()                   # jobs that not finished during [prev_time, cur_time)
        self.prev_time = 0                  # previous reward calculation time

    def get_reward(self, jobs, cur_time):
        reward = 0
        for job in jobs:
            self.jobs.add(job)

        if args.learn_obj == 'mean':
            for job in list(self.jobs):
                reward -= (min(job.finish_time, cur_time) - max(job.start_time, self.prev_time)) / args.reward_scale
                if job.finished:
                    self.jobs.remove(job)
        elif args.learn_job == 'makespan':
            reward -= (cur_time - self.prev_time) / args.reward_scale
        else:
            print('Unsupported learn object!')
            exit(1)

        self.prev_time = cur_time
        return reward

    def reset(self):
        self.jobs.clear()
        self.prev_time = 0


class ExecutorCommit:
    """
    self.commit is a dict, where each key-value pair is {stage/job: OrderedDict(stage: amount)}.
    The key is the previously served stage/job of the first exec in self.exec_to_schedule, the value is
    an ordered dict where
        - the key is the next to-be-scheduled stage, and
        - the value is the num of executors allocated to it.
    """
    def __init__(self):
        self.commit = {}             # {src stage/job: OrderedDict(next_stage: amount of allocated execs)}
        self.stage_commit = {}       # {next_stage: amount of allocated execs}
        self.backward = {}           # {next_stage: set(src stages/jobs)}

    def __getitem__(self, src):
        return self.commit[src]

    def add(self, src, stage, amount):
        """
        Add a scheduling commitment.
        :param src: the first to-be-scheduled exec previously served job (or stage)
        :param stage: the next-to-schedule stage returned from the scheduling algorithm (agent)
        :param amount: the amount of execs need to allocate to the next-to-schedule stage
        (calculated according to the limit returned from the scheduling algorithm (agent))
        """
        # if non-exist then create
        if stage not in self.commit[src]:
            self.commit[src][stage] = 0
        # add
        self.commit[src][stage] += amount
        self.stage_commit[stage] += amount
        self.backward[stage].add(src)

    def pop(self, src):
        assert src in self.commit
        assert len(self.commit[src]) > 0

        stage = next(iter(self.commit[src]))
        # deduct
        self.commit[src][stage] -= 1
        self.stage_commit[stage] -= 1
        assert self.commit[src][stage] >= 0
        assert self.stage_commit[stage] >= 0
        # remove if amount is zero
        if self.commit[src][stage] == 0:
            del self.commit[src][stage]
            self.backward[stage].remove(src)

        return stage

    def add_job(self, job):
        self.commit[job] = OrderedDict()
        for stage in job.stages:
            self.commit[stage] = OrderedDict()
            self.stage_commit[stage] = 0
            self.backward[stage] = set()

    def remove_job(self, job):
        assert len(self.commit[job]) == 0
        del self.commit[job]
        for stage in job.stages:
            assert len(self.commit[stage]) == 0
            del self.commit[stage]

            for src in self.backward[stage]:
                del self.commit[src][stage]
            del self.backward[stage]
            del self.stage_commit[stage]

    def reset(self):
        self.commit = {None: OrderedDict()}
        self.stage_commit = {None: 0}
        self.backward = {None: set()}


def get_act2stage(jobs):
    """
    Get the translation from the action (an integer between [0, num_stages_in_all_jobs]) to the corresponding stage.
    """
    act2stage = utils.ReversibleMap()
    act = 0
    for job in jobs:
        for stage in job.stages:
            act2stage[act] = stage     # because of the __setitem__ function
            act += 1
    return act2stage
